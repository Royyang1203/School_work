{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.1+cu116\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = str(Path(fr\"./data/train.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 64\n",
    "# define the input size of the model\n",
    "input_size = 160\n",
    "epoch_num = 300\n",
    "dtype = torch.float\n",
    "\n",
    "# Learning Rate\n",
    "LR = 1e-3\n",
    "EPS = 1e-7\n",
    "LR_STEP = 100\n",
    "LR_GAMMA = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        data = pd.read_csv(data_dir)\n",
    "        self.inputs = data[['x1', 'x2']].to_numpy()\n",
    "        self.outputs = data['y'].to_numpy()\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_data = torch.tensor(self.inputs[idx], dtype= torch.float)\n",
    "        output_data = torch.tensor(self.outputs[idx], dtype= torch.float)\n",
    "\n",
    "        if self.transform:\n",
    "            input_data = self.transform(input_data)\n",
    "\n",
    "        return input_data, output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(data_dir)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DeepModel(nn.Module):\n",
    "#     def __init__(self, layer_sizes):\n",
    "#         super(DeepModel, self).__init__()\n",
    "\n",
    "#         self.layers = nn.ModuleList()\n",
    "\n",
    "#         for i in range(len(layer_sizes) - 1):\n",
    "#             layer = nn.Linear(layer_sizes[i], layer_sizes[i + 1])\n",
    "#             init.xavier_uniform_(layer.weight)  # 使用Xavier初始化\n",
    "#             self.layers.append(layer)\n",
    "\n",
    "#         self.activation = nn.LeakyReLU()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         for layer in self.layers[:-1]:\n",
    "#             x = self.activation(layer(x))\n",
    "        \n",
    "#         x = self.layers[-1](x)  # 最后一层不使用激活函数\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepModelWithBN(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            layer_sizes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            layer = nn.Linear(layer_sizes[i], layer_sizes[i + 1]).to(dtype)\n",
    "            # nn.init.xavier_uniform_(layer.weight)  # 使用Xavier初始化\n",
    "            self.layers.append(layer)\n",
    "            \n",
    "            # if i != len(layer_sizes) - 2:  # 不在最后一个线性层后使用批量归一化\n",
    "            #     bn = nn.BatchNorm1d(layer_sizes[i + 1]).to(dtype)\n",
    "            #     self.layers.append(bn)\n",
    "\n",
    "        # self.activation = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers[:-1]):\n",
    "            x = layer(x)\n",
    "            # if isinstance(layer, nn.Linear):  # 只在线性层后使用激活函数\n",
    "            #     x = self.activation(x)\n",
    "        \n",
    "        x = self.layers[-1](x)  # 最后一层不使用激活函数\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EasyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(2, 128)\n",
    "        self.layer2 = nn.Linear(128, 1)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.layer1(x))\n",
    "        x = self.layer2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = DeepModelWithBN([2, 16, 1])\n",
    "model = EasyModel()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, eps=EPS)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "# lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=LR_STEP, gamma=LR_GAMMA)\n",
    "criterion = nn.MSELoss()\n",
    "# criterion = nn.L1Loss()\n",
    "# criterion = nn.SmoothL1Loss()\n",
    "# criterion = nn.PoissonNLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/300, Train Loss: 20.59938539123535, Validation Loss: 10.060813674926758\n",
      "Epoch: 2/300, Train Loss: 9.053247436523437, Validation Loss: 8.570980491638183\n",
      "Epoch: 3/300, Train Loss: 8.095714611053467, Validation Loss: 7.986491680145264\n",
      "Epoch: 4/300, Train Loss: 7.680414859771728, Validation Loss: 7.684364547729492\n",
      "Epoch: 5/300, Train Loss: 7.410926628112793, Validation Loss: 7.424878330230713\n",
      "Epoch: 6/300, Train Loss: 7.209316089630127, Validation Loss: 7.231489810943604\n",
      "Epoch: 7/300, Train Loss: 7.025253211975097, Validation Loss: 7.040695743560791\n",
      "Epoch: 8/300, Train Loss: 6.866330814361572, Validation Loss: 6.957788200378418\n",
      "Epoch: 9/300, Train Loss: 6.744995132446289, Validation Loss: 6.786459712982178\n",
      "Epoch: 10/300, Train Loss: 6.604659015655518, Validation Loss: 6.684731388092041\n",
      "Epoch: 11/300, Train Loss: 6.496509456634522, Validation Loss: 6.481972274780273\n",
      "Epoch: 12/300, Train Loss: 6.325773052215576, Validation Loss: 6.405028972625733\n",
      "Epoch: 13/300, Train Loss: 6.232822925567627, Validation Loss: 6.223314914703369\n",
      "Epoch: 14/300, Train Loss: 6.137751434326172, Validation Loss: 6.160223369598389\n",
      "Epoch: 15/300, Train Loss: 6.0107536888122555, Validation Loss: 6.0494953155517575\n",
      "Epoch: 16/300, Train Loss: 5.902706840515137, Validation Loss: 5.990805702209473\n",
      "Epoch: 17/300, Train Loss: 5.816101526260376, Validation Loss: 5.901068172454834\n",
      "Epoch: 18/300, Train Loss: 5.761714328765869, Validation Loss: 5.825037937164307\n",
      "Epoch: 19/300, Train Loss: 5.68959845161438, Validation Loss: 5.710853385925293\n",
      "Epoch: 20/300, Train Loss: 5.551679660797119, Validation Loss: 5.5887355041503906\n",
      "Epoch: 21/300, Train Loss: 5.521007987976074, Validation Loss: 5.478456897735596\n",
      "Epoch: 22/300, Train Loss: 5.4457242240905765, Validation Loss: 5.4192852210998534\n",
      "Epoch: 23/300, Train Loss: 5.369142864227295, Validation Loss: 5.3330471801757815\n",
      "Epoch: 24/300, Train Loss: 5.2944786167144775, Validation Loss: 5.405070896148682\n",
      "Epoch: 25/300, Train Loss: 5.243695684432984, Validation Loss: 5.259385433197021\n",
      "Epoch: 26/300, Train Loss: 5.17790160369873, Validation Loss: 5.394961566925049\n",
      "Epoch: 27/300, Train Loss: 5.124280090332031, Validation Loss: 5.136166801452637\n",
      "Epoch: 28/300, Train Loss: 5.074853958129883, Validation Loss: 5.026055393218994\n",
      "Epoch: 29/300, Train Loss: 5.024034677505493, Validation Loss: 4.976706161499023\n",
      "Epoch: 30/300, Train Loss: 4.964630014419556, Validation Loss: 4.9672754287719725\n",
      "Epoch: 31/300, Train Loss: 4.921543054580688, Validation Loss: 4.933785791397095\n",
      "Epoch: 32/300, Train Loss: 4.882267099380493, Validation Loss: 4.919564075469971\n",
      "Epoch: 33/300, Train Loss: 4.813653699874878, Validation Loss: 4.776486358642578\n",
      "Epoch: 34/300, Train Loss: 4.765336267471313, Validation Loss: 4.8350882244110105\n",
      "Epoch: 35/300, Train Loss: 4.775285055160523, Validation Loss: 4.791519660949707\n",
      "Epoch: 36/300, Train Loss: 4.708478397369385, Validation Loss: 4.7872438144683835\n",
      "Epoch: 37/300, Train Loss: 4.677082469940186, Validation Loss: 4.669357814788818\n",
      "Epoch: 38/300, Train Loss: 4.653696269989013, Validation Loss: 4.742426023483277\n",
      "Epoch: 39/300, Train Loss: 4.590866510391235, Validation Loss: 4.629349536895752\n",
      "Epoch: 40/300, Train Loss: 4.5610077133178715, Validation Loss: 4.500016374588013\n",
      "Epoch: 41/300, Train Loss: 4.534916091918945, Validation Loss: 4.446093006134033\n",
      "Epoch: 42/300, Train Loss: 4.5400518074035645, Validation Loss: 4.62693883895874\n",
      "Epoch: 43/300, Train Loss: 4.488588283538818, Validation Loss: 4.484007015228271\n",
      "Epoch: 44/300, Train Loss: 4.403771818161011, Validation Loss: 4.491313037872314\n",
      "Epoch: 45/300, Train Loss: 4.436017992019654, Validation Loss: 4.458723812103272\n",
      "Epoch: 46/300, Train Loss: 4.368576875686646, Validation Loss: 4.355121622085571\n",
      "Epoch: 47/300, Train Loss: 4.351691822052002, Validation Loss: 4.282980260848999\n",
      "Epoch: 48/300, Train Loss: 4.353851816177368, Validation Loss: 4.320699405670166\n",
      "Epoch: 49/300, Train Loss: 4.275463224411011, Validation Loss: 4.188924417495728\n",
      "Epoch: 50/300, Train Loss: 4.247574968338013, Validation Loss: 4.249425630569458\n",
      "Epoch: 51/300, Train Loss: 4.240898990631104, Validation Loss: 4.174759836196899\n",
      "Epoch: 52/300, Train Loss: 4.194435796737671, Validation Loss: 4.201128196716309\n",
      "Epoch: 53/300, Train Loss: 4.202063770294189, Validation Loss: 4.211119527816773\n",
      "Epoch: 54/300, Train Loss: 4.156991792678833, Validation Loss: 4.106837797164917\n",
      "Epoch: 55/300, Train Loss: 4.129947317123413, Validation Loss: 4.073110074996948\n",
      "Epoch: 56/300, Train Loss: 4.117353834152222, Validation Loss: 4.009195032119751\n",
      "Epoch: 57/300, Train Loss: 4.055030275344849, Validation Loss: 4.022182636260986\n",
      "Epoch: 58/300, Train Loss: 4.069316049575805, Validation Loss: 3.9908439826965334\n",
      "Epoch: 59/300, Train Loss: 4.021337459564209, Validation Loss: 4.024695920944214\n",
      "Epoch: 60/300, Train Loss: 3.9847465362548826, Validation Loss: 3.939245538711548\n",
      "Epoch: 61/300, Train Loss: 3.9755209465026855, Validation Loss: 4.116443376541138\n",
      "Epoch: 62/300, Train Loss: 3.9811134967803956, Validation Loss: 3.9089229202270506\n",
      "Epoch: 63/300, Train Loss: 3.933128629684448, Validation Loss: 3.8608946228027343\n",
      "Epoch: 64/300, Train Loss: 3.9045456295013428, Validation Loss: 3.8070993328094485\n",
      "Epoch: 65/300, Train Loss: 3.8870311546325684, Validation Loss: 3.8102143573760987\n",
      "Epoch: 66/300, Train Loss: 3.854299283981323, Validation Loss: 3.7633489036560057\n",
      "Epoch: 67/300, Train Loss: 3.848502103805542, Validation Loss: 3.775631895065308\n",
      "Epoch: 68/300, Train Loss: 3.839216556549072, Validation Loss: 3.7292657566070555\n",
      "Epoch: 69/300, Train Loss: 3.778919584274292, Validation Loss: 3.703646183013916\n",
      "Epoch: 70/300, Train Loss: 3.8058512897491457, Validation Loss: 3.708858289718628\n",
      "Epoch: 71/300, Train Loss: 3.7556170806884768, Validation Loss: 3.6482758712768555\n",
      "Epoch: 72/300, Train Loss: 3.742967113494873, Validation Loss: 3.6663371181488036\n",
      "Epoch: 73/300, Train Loss: 3.6904439544677734, Validation Loss: 3.8039810371398928\n",
      "Epoch: 74/300, Train Loss: 3.72316415977478, Validation Loss: 3.6013778686523437\n",
      "Epoch: 75/300, Train Loss: 3.7085894985198973, Validation Loss: 3.6409626960754395\n",
      "Epoch: 76/300, Train Loss: 3.645605417251587, Validation Loss: 3.5702062702178954\n",
      "Epoch: 77/300, Train Loss: 3.6457825260162355, Validation Loss: 3.5270586109161375\n",
      "Epoch: 78/300, Train Loss: 3.6220291481018068, Validation Loss: 3.486760206222534\n",
      "Epoch: 79/300, Train Loss: 3.5759221115112303, Validation Loss: 3.497931032180786\n",
      "Epoch: 80/300, Train Loss: 3.581009769439697, Validation Loss: 3.4903619480133057\n",
      "Epoch: 81/300, Train Loss: 3.574921339035034, Validation Loss: 3.4413232612609863\n",
      "Epoch: 82/300, Train Loss: 3.53525018119812, Validation Loss: 3.4777395725250244\n",
      "Epoch: 83/300, Train Loss: 3.5455004997253416, Validation Loss: 3.415000877380371\n",
      "Epoch: 84/300, Train Loss: 3.52897767829895, Validation Loss: 3.5026803684234618\n",
      "Epoch: 85/300, Train Loss: 3.5324121456146242, Validation Loss: 3.3842352199554444\n",
      "Epoch: 86/300, Train Loss: 3.467477672576904, Validation Loss: 3.501779556274414\n",
      "Epoch: 87/300, Train Loss: 3.4791716384887694, Validation Loss: 3.5004545307159423\n",
      "Epoch: 88/300, Train Loss: 3.4570793933868407, Validation Loss: 3.382524938583374\n",
      "Epoch: 89/300, Train Loss: 3.4262170219421386, Validation Loss: 3.313807973861694\n",
      "Epoch: 90/300, Train Loss: 3.4230867080688476, Validation Loss: 3.3235247325897217\n",
      "Epoch: 91/300, Train Loss: 3.3809702014923095, Validation Loss: 3.3553399658203125\n",
      "Epoch: 92/300, Train Loss: 3.3747630920410154, Validation Loss: 3.338832263946533\n",
      "Epoch: 93/300, Train Loss: 3.3469248752593996, Validation Loss: 3.2859209823608397\n",
      "Epoch: 94/300, Train Loss: 3.355672311782837, Validation Loss: 3.3029550647735597\n",
      "Epoch: 95/300, Train Loss: 3.385935694694519, Validation Loss: 3.5053271579742433\n",
      "Epoch: 96/300, Train Loss: 3.34461910533905, Validation Loss: 3.2311853885650637\n",
      "Epoch: 97/300, Train Loss: 3.281121563911438, Validation Loss: 3.22157995223999\n",
      "Epoch: 98/300, Train Loss: 3.2631226596832277, Validation Loss: 3.2387352275848387\n",
      "Epoch: 99/300, Train Loss: 3.2584909534454347, Validation Loss: 3.1560556411743166\n",
      "Epoch: 100/300, Train Loss: 3.249379521369934, Validation Loss: 3.242108917236328\n",
      "Epoch: 101/300, Train Loss: 3.2215560598373414, Validation Loss: 3.168622455596924\n",
      "Epoch: 102/300, Train Loss: 3.2295847759246827, Validation Loss: 3.135424213409424\n",
      "Epoch: 103/300, Train Loss: 3.21199725151062, Validation Loss: 3.102272529602051\n",
      "Epoch: 104/300, Train Loss: 3.1992211790084837, Validation Loss: 3.1608712577819826\n",
      "Epoch: 105/300, Train Loss: 3.182181577682495, Validation Loss: 3.18739727973938\n",
      "Epoch: 106/300, Train Loss: 3.1609193773269655, Validation Loss: 3.161811227798462\n",
      "Epoch: 107/300, Train Loss: 3.11746245765686, Validation Loss: 3.0799754524230956\n",
      "Epoch: 108/300, Train Loss: 3.1298109283447264, Validation Loss: 3.0230839252471924\n",
      "Epoch: 109/300, Train Loss: 3.113316547393799, Validation Loss: 3.014787445068359\n",
      "Epoch: 110/300, Train Loss: 3.0976677465438844, Validation Loss: 2.9867354583740235\n",
      "Epoch: 111/300, Train Loss: 3.0813442134857176, Validation Loss: 2.9744832134246826\n",
      "Epoch: 112/300, Train Loss: 3.0446573038101197, Validation Loss: 3.05309868812561\n",
      "Epoch: 113/300, Train Loss: 3.0509601821899412, Validation Loss: 2.975575637817383\n",
      "Epoch: 114/300, Train Loss: 3.025716389656067, Validation Loss: 2.969409313201904\n",
      "Epoch: 115/300, Train Loss: 3.060416513442993, Validation Loss: 3.001564245223999\n",
      "Epoch: 116/300, Train Loss: 2.9912653799057005, Validation Loss: 2.896365175247192\n",
      "Epoch: 117/300, Train Loss: 2.962648042678833, Validation Loss: 2.93848295211792\n",
      "Epoch: 118/300, Train Loss: 2.9595263023376464, Validation Loss: 2.9241656684875488\n",
      "Epoch: 119/300, Train Loss: 2.9489885969161986, Validation Loss: 2.862621603012085\n",
      "Epoch: 120/300, Train Loss: 2.927979380607605, Validation Loss: 2.9660372734069824\n",
      "Epoch: 121/300, Train Loss: 2.911118483543396, Validation Loss: 2.9252363872528075\n",
      "Epoch: 122/300, Train Loss: 2.8824073781967163, Validation Loss: 2.832128438949585\n",
      "Epoch: 123/300, Train Loss: 2.902710096359253, Validation Loss: 2.9375378322601318\n",
      "Epoch: 124/300, Train Loss: 2.8726982755661012, Validation Loss: 2.7774074506759643\n",
      "Epoch: 125/300, Train Loss: 2.8609530572891235, Validation Loss: 2.823455629348755\n",
      "Epoch: 126/300, Train Loss: 2.830080156326294, Validation Loss: 2.8636813449859617\n",
      "Epoch: 127/300, Train Loss: 2.843572275161743, Validation Loss: 2.7110429286956785\n",
      "Epoch: 128/300, Train Loss: 2.8075697717666626, Validation Loss: 2.709533052444458\n",
      "Epoch: 129/300, Train Loss: 2.784532545089722, Validation Loss: 2.8067471504211428\n",
      "Epoch: 130/300, Train Loss: 2.789454531669617, Validation Loss: 2.677846727371216\n",
      "Epoch: 131/300, Train Loss: 2.75645329284668, Validation Loss: 2.7570886516571047\n",
      "Epoch: 132/300, Train Loss: 2.768461290359497, Validation Loss: 2.666481957435608\n",
      "Epoch: 133/300, Train Loss: 2.7449372100830076, Validation Loss: 2.655680274963379\n",
      "Epoch: 134/300, Train Loss: 2.7185279302597047, Validation Loss: 2.6027653217315674\n",
      "Epoch: 135/300, Train Loss: 2.7353665504455567, Validation Loss: 2.6190028381347656\n",
      "Epoch: 136/300, Train Loss: 2.682920401573181, Validation Loss: 2.73319167137146\n",
      "Epoch: 137/300, Train Loss: 2.683178424835205, Validation Loss: 2.62628529548645\n",
      "Epoch: 138/300, Train Loss: 2.671575291633606, Validation Loss: 2.6758397579193116\n",
      "Epoch: 139/300, Train Loss: 2.6406321802139283, Validation Loss: 2.5875159072875977\n",
      "Epoch: 140/300, Train Loss: 2.6362705030441282, Validation Loss: 2.5684876537322996\n",
      "Epoch: 141/300, Train Loss: 2.6251625146865845, Validation Loss: 2.6131800556182863\n",
      "Epoch: 142/300, Train Loss: 2.6531485319137573, Validation Loss: 2.8042406749725344\n",
      "Epoch: 143/300, Train Loss: 2.624477780342102, Validation Loss: 2.519863638877869\n",
      "Epoch: 144/300, Train Loss: 2.586232672691345, Validation Loss: 2.5615835905075075\n",
      "Epoch: 145/300, Train Loss: 2.5778384618759156, Validation Loss: 2.5524986839294432\n",
      "Epoch: 146/300, Train Loss: 2.5761482095718384, Validation Loss: 2.473842153549194\n",
      "Epoch: 147/300, Train Loss: 2.574902521133423, Validation Loss: 2.817691855430603\n",
      "Epoch: 148/300, Train Loss: 2.5602784938812255, Validation Loss: 2.5040740966796875\n",
      "Epoch: 149/300, Train Loss: 2.555324337005615, Validation Loss: 2.480516581535339\n",
      "Epoch: 150/300, Train Loss: 2.5942789478302, Validation Loss: 2.4400429487228394\n",
      "Epoch: 151/300, Train Loss: 2.542618025779724, Validation Loss: 2.5063154697418213\n",
      "Epoch: 152/300, Train Loss: 2.514639391899109, Validation Loss: 2.4768502950668334\n",
      "Epoch: 153/300, Train Loss: 2.522672631263733, Validation Loss: 2.4547005987167356\n",
      "Epoch: 154/300, Train Loss: 2.524106463432312, Validation Loss: 2.4886506175994874\n",
      "Epoch: 155/300, Train Loss: 2.4996442909240724, Validation Loss: 2.39785409450531\n",
      "Epoch: 156/300, Train Loss: 2.5026335248947142, Validation Loss: 2.4110438203811646\n",
      "Epoch: 157/300, Train Loss: 2.4736930065155027, Validation Loss: 2.447988901138306\n",
      "Epoch: 158/300, Train Loss: 2.451108371734619, Validation Loss: 2.4229666137695314\n",
      "Epoch: 159/300, Train Loss: 2.457428581237793, Validation Loss: 2.5539401245117186\n",
      "Epoch: 160/300, Train Loss: 2.471547720909119, Validation Loss: 2.3913396072387694\n",
      "Epoch: 161/300, Train Loss: 2.4364986419677734, Validation Loss: 2.37975257396698\n",
      "Epoch: 162/300, Train Loss: 2.4380507307052612, Validation Loss: 2.37201096534729\n",
      "Epoch: 163/300, Train Loss: 2.4401143693923952, Validation Loss: 2.426519527435303\n",
      "Epoch: 164/300, Train Loss: 2.425880286216736, Validation Loss: 2.3307888507843018\n",
      "Epoch: 165/300, Train Loss: 2.4063281831741334, Validation Loss: 2.5297017192840574\n",
      "Epoch: 166/300, Train Loss: 2.4543414478302004, Validation Loss: 2.409048318862915\n",
      "Epoch: 167/300, Train Loss: 2.4192168912887575, Validation Loss: 2.6317247772216796\n",
      "Epoch: 168/300, Train Loss: 2.413235160827637, Validation Loss: 2.371074161529541\n",
      "Epoch: 169/300, Train Loss: 2.3854446449279787, Validation Loss: 2.3980283975601195\n",
      "Epoch: 170/300, Train Loss: 2.399234040260315, Validation Loss: 2.4511670732498168\n",
      "Epoch: 171/300, Train Loss: 2.4052612438201906, Validation Loss: 2.3202532815933226\n",
      "Epoch: 172/300, Train Loss: 2.369368574142456, Validation Loss: 2.303487191200256\n",
      "Epoch: 173/300, Train Loss: 2.3603407888412478, Validation Loss: 2.2641388845443724\n",
      "Epoch: 174/300, Train Loss: 2.3594010982513427, Validation Loss: 2.3321734285354614\n",
      "Epoch: 175/300, Train Loss: 2.3666545152664185, Validation Loss: 2.3058559465408326\n",
      "Epoch: 176/300, Train Loss: 2.355067828178406, Validation Loss: 2.2886977672576903\n",
      "Epoch: 177/300, Train Loss: 2.348589566230774, Validation Loss: 2.303752236366272\n",
      "Epoch: 178/300, Train Loss: 2.3427186365127564, Validation Loss: 2.4138852024078368\n",
      "Epoch: 179/300, Train Loss: 2.3291906042099, Validation Loss: 2.2500175619125367\n",
      "Epoch: 180/300, Train Loss: 2.3342714385986327, Validation Loss: 2.298742160797119\n",
      "Epoch: 181/300, Train Loss: 2.316951672554016, Validation Loss: 2.2267034435272217\n",
      "Epoch: 182/300, Train Loss: 2.3104164247512817, Validation Loss: 2.3282665634155273\n",
      "Epoch: 183/300, Train Loss: 2.353017681121826, Validation Loss: 2.290460829734802\n",
      "Epoch: 184/300, Train Loss: 2.315565574645996, Validation Loss: 2.3789591073989866\n",
      "Epoch: 185/300, Train Loss: 2.32948321723938, Validation Loss: 2.228304400444031\n",
      "Epoch: 186/300, Train Loss: 2.2903171644210816, Validation Loss: 2.2321418428421023\n",
      "Epoch: 187/300, Train Loss: 2.288663004875183, Validation Loss: 2.2300273895263674\n",
      "Epoch: 188/300, Train Loss: 2.3126679496765137, Validation Loss: 2.28597083568573\n",
      "Epoch: 189/300, Train Loss: 2.328460838317871, Validation Loss: 2.265863881111145\n",
      "Epoch: 190/300, Train Loss: 2.2811626205444338, Validation Loss: 2.303152093887329\n",
      "Epoch: 191/300, Train Loss: 2.305188472747803, Validation Loss: 2.1996042776107787\n",
      "Epoch: 192/300, Train Loss: 2.3036752729415895, Validation Loss: 2.342639808654785\n",
      "Epoch: 193/300, Train Loss: 2.27231676197052, Validation Loss: 2.2730544662475585\n",
      "Epoch: 194/300, Train Loss: 2.247152843475342, Validation Loss: 2.2875853538513184\n",
      "Epoch: 195/300, Train Loss: 2.3292207384109496, Validation Loss: 2.253602313995361\n",
      "Epoch: 196/300, Train Loss: 2.2532723426818846, Validation Loss: 2.1858187341690063\n",
      "Epoch: 197/300, Train Loss: 2.2480253267288206, Validation Loss: 2.1974389505386354\n",
      "Epoch: 198/300, Train Loss: 2.276291814804077, Validation Loss: 2.1876983213424683\n",
      "Epoch: 199/300, Train Loss: 2.266564150810242, Validation Loss: 2.30222375869751\n",
      "Epoch: 200/300, Train Loss: 2.2300280389785767, Validation Loss: 2.1989437341690063\n",
      "Epoch: 201/300, Train Loss: 2.233101647377014, Validation Loss: 2.1826559686660767\n",
      "Epoch: 202/300, Train Loss: 2.266701887130737, Validation Loss: 2.4999789714813234\n",
      "Epoch: 203/300, Train Loss: 2.2446785163879395, Validation Loss: 2.1722887754440308\n",
      "Epoch: 204/300, Train Loss: 2.2214272384643556, Validation Loss: 2.1630850505828856\n",
      "Epoch: 205/300, Train Loss: 2.2668812141418457, Validation Loss: 2.202224645614624\n",
      "Epoch: 206/300, Train Loss: 2.222034873008728, Validation Loss: 2.1663834047317505\n",
      "Epoch: 207/300, Train Loss: 2.2277086391448973, Validation Loss: 2.1690855169296266\n",
      "Epoch: 208/300, Train Loss: 2.199542261123657, Validation Loss: 2.152278389930725\n",
      "Epoch: 209/300, Train Loss: 2.1898608875274657, Validation Loss: 2.18205979347229\n",
      "Epoch: 210/300, Train Loss: 2.217322792053223, Validation Loss: 2.203627028465271\n",
      "Epoch: 211/300, Train Loss: 2.209788390159607, Validation Loss: 2.2351741075515745\n",
      "Epoch: 212/300, Train Loss: 2.2094233112335204, Validation Loss: 2.181004285812378\n",
      "Epoch: 213/300, Train Loss: 2.1962577657699587, Validation Loss: 2.1799029159545897\n",
      "Epoch: 214/300, Train Loss: 2.2161374568939207, Validation Loss: 2.210863723754883\n",
      "Epoch: 215/300, Train Loss: 2.1926192665100097, Validation Loss: 2.1507620859146117\n",
      "Epoch: 216/300, Train Loss: 2.199060875892639, Validation Loss: 2.2108399152755736\n",
      "Epoch: 217/300, Train Loss: 2.1748697881698607, Validation Loss: 2.2036284017562866\n",
      "Epoch: 218/300, Train Loss: 2.186117583274841, Validation Loss: 2.084637680053711\n",
      "Epoch: 219/300, Train Loss: 2.16741090965271, Validation Loss: 2.25751202583313\n",
      "Epoch: 220/300, Train Loss: 2.1812176666259764, Validation Loss: 2.1651333284378054\n",
      "Epoch: 221/300, Train Loss: 2.205700109004974, Validation Loss: 2.273107481002808\n",
      "Epoch: 222/300, Train Loss: 2.190177803993225, Validation Loss: 2.185234923362732\n",
      "Epoch: 223/300, Train Loss: 2.198400333404541, Validation Loss: 2.1245698070526124\n",
      "Epoch: 224/300, Train Loss: 2.1974443893432616, Validation Loss: 2.094970774650574\n",
      "Epoch: 225/300, Train Loss: 2.1768132266998292, Validation Loss: 2.075559210777283\n",
      "Epoch: 226/300, Train Loss: 2.190207445144653, Validation Loss: 2.099250454902649\n",
      "Epoch: 227/300, Train Loss: 2.1702926054000855, Validation Loss: 2.22408757686615\n",
      "Epoch: 228/300, Train Loss: 2.164720242500305, Validation Loss: 2.138127784729004\n",
      "Epoch: 229/300, Train Loss: 2.1547843770980837, Validation Loss: 2.0837787342071534\n",
      "Epoch: 230/300, Train Loss: 2.1559065942764284, Validation Loss: 2.085630793571472\n",
      "Epoch: 231/300, Train Loss: 2.1528888969421387, Validation Loss: 2.057500524520874\n",
      "Epoch: 232/300, Train Loss: 2.1319821138381956, Validation Loss: 2.0827210521698\n",
      "Epoch: 233/300, Train Loss: 2.160016779899597, Validation Loss: 2.147252469062805\n",
      "Epoch: 234/300, Train Loss: 2.1597426013946532, Validation Loss: 2.115152521133423\n",
      "Epoch: 235/300, Train Loss: 2.117127040863037, Validation Loss: 2.066830625534058\n",
      "Epoch: 236/300, Train Loss: 2.1499129209518433, Validation Loss: 2.090372071266174\n",
      "Epoch: 237/300, Train Loss: 2.128226453781128, Validation Loss: 2.129657254219055\n",
      "Epoch: 238/300, Train Loss: 2.138412854194641, Validation Loss: 2.065306406021118\n",
      "Epoch: 239/300, Train Loss: 2.1283781309127807, Validation Loss: 2.138792386054993\n",
      "Epoch: 240/300, Train Loss: 2.145841423988342, Validation Loss: 2.085999526977539\n",
      "Epoch: 241/300, Train Loss: 2.122263259887695, Validation Loss: 2.064202318191528\n",
      "Epoch: 242/300, Train Loss: 2.1516746454238893, Validation Loss: 2.038022589683533\n",
      "Epoch: 243/300, Train Loss: 2.1590184526443483, Validation Loss: 2.072243194580078\n",
      "Epoch: 244/300, Train Loss: 2.1215204677581787, Validation Loss: 2.0638258504867553\n",
      "Epoch: 245/300, Train Loss: 2.1087516355514526, Validation Loss: 2.0723081827163696\n",
      "Epoch: 246/300, Train Loss: 2.1453571615219116, Validation Loss: 2.2578492546081543\n",
      "Epoch: 247/300, Train Loss: 2.1268824520111083, Validation Loss: 2.059715538024902\n",
      "Epoch: 248/300, Train Loss: 2.0935266189575197, Validation Loss: 2.038597345352173\n",
      "Epoch: 249/300, Train Loss: 2.0898702220916747, Validation Loss: 2.103526406288147\n",
      "Epoch: 250/300, Train Loss: 2.117214069366455, Validation Loss: 2.2000854301452635\n",
      "Epoch: 251/300, Train Loss: 2.1188616199493406, Validation Loss: 2.049477872848511\n",
      "Epoch: 252/300, Train Loss: 2.0969808712005613, Validation Loss: 2.0424265098571777\n",
      "Epoch: 253/300, Train Loss: 2.0974345836639405, Validation Loss: 2.064297709465027\n",
      "Epoch: 254/300, Train Loss: 2.1383087186813357, Validation Loss: 2.082103705406189\n",
      "Epoch: 255/300, Train Loss: 2.1000606203079224, Validation Loss: 2.119809460639954\n",
      "Epoch: 256/300, Train Loss: 2.0854990940093994, Validation Loss: 2.0340241098403933\n",
      "Epoch: 257/300, Train Loss: 2.1273550176620484, Validation Loss: 2.0150350618362425\n",
      "Epoch: 258/300, Train Loss: 2.0964294872283937, Validation Loss: 2.1075380182266237\n",
      "Epoch: 259/300, Train Loss: 2.0772102699279786, Validation Loss: 2.065278024673462\n",
      "Epoch: 260/300, Train Loss: 2.104966502189636, Validation Loss: 2.114492483139038\n",
      "Epoch: 261/300, Train Loss: 2.0819819984436037, Validation Loss: 2.058899621963501\n",
      "Epoch: 262/300, Train Loss: 2.0854699506759644, Validation Loss: 2.003899292945862\n",
      "Epoch: 263/300, Train Loss: 2.0823994722366335, Validation Loss: 2.111046624183655\n",
      "Epoch: 264/300, Train Loss: 2.1111758632659914, Validation Loss: 2.0920225620269775\n",
      "Epoch: 265/300, Train Loss: 2.1344298849105834, Validation Loss: 2.016341710090637\n",
      "Epoch: 266/300, Train Loss: 2.091483470916748, Validation Loss: 2.0885296869277954\n",
      "Epoch: 267/300, Train Loss: 2.090131628036499, Validation Loss: 2.057497010231018\n",
      "Epoch: 268/300, Train Loss: 2.0842175455093384, Validation Loss: 2.014283633232117\n",
      "Epoch: 269/300, Train Loss: 2.086867931365967, Validation Loss: 2.0598928213119505\n",
      "Epoch: 270/300, Train Loss: 2.062521800041199, Validation Loss: 2.0763741064071657\n",
      "Epoch: 271/300, Train Loss: 2.0828977136611937, Validation Loss: 2.0056115436553954\n",
      "Epoch: 272/300, Train Loss: 2.0792172484397886, Validation Loss: 2.04458860874176\n",
      "Epoch: 273/300, Train Loss: 2.055791986465454, Validation Loss: 2.0653483724594115\n",
      "Epoch: 274/300, Train Loss: 2.076098554611206, Validation Loss: 1.9936801147460939\n",
      "Epoch: 275/300, Train Loss: 2.109608988761902, Validation Loss: 1.9969508981704711\n",
      "Epoch: 276/300, Train Loss: 2.094462742805481, Validation Loss: 2.0673336791992187\n",
      "Epoch: 277/300, Train Loss: 2.0979592599868773, Validation Loss: 2.0424792909622194\n",
      "Epoch: 278/300, Train Loss: 2.0402947864532472, Validation Loss: 2.0042280626296995\n",
      "Epoch: 279/300, Train Loss: 2.0699098205566404, Validation Loss: 2.118417139053345\n",
      "Epoch: 280/300, Train Loss: 2.071339974403381, Validation Loss: 2.1595826005935668\n",
      "Epoch: 281/300, Train Loss: 2.0798356456756593, Validation Loss: 2.1399858713150026\n",
      "Epoch: 282/300, Train Loss: 2.0609936971664427, Validation Loss: 2.074084801673889\n",
      "Epoch: 283/300, Train Loss: 2.048103904724121, Validation Loss: 1.9887126731872558\n",
      "Epoch: 284/300, Train Loss: 2.0561901445388795, Validation Loss: 2.0249302768707276\n",
      "Epoch: 285/300, Train Loss: 2.0736866302490236, Validation Loss: 1.9703428506851197\n",
      "Epoch: 286/300, Train Loss: 2.0450814008712768, Validation Loss: 2.0701799392700195\n",
      "Epoch: 287/300, Train Loss: 2.1383174362182618, Validation Loss: 1.9851276540756226\n",
      "Epoch: 288/300, Train Loss: 2.050067844390869, Validation Loss: 1.9707399940490722\n",
      "Epoch: 289/300, Train Loss: 2.079795207977295, Validation Loss: 1.9957477474212646\n",
      "Epoch: 290/300, Train Loss: 2.043649573326111, Validation Loss: 2.048039174079895\n",
      "Epoch: 291/300, Train Loss: 2.05556397151947, Validation Loss: 1.9617167186737061\n",
      "Epoch: 292/300, Train Loss: 2.049814368247986, Validation Loss: 2.157236671447754\n",
      "Epoch: 293/300, Train Loss: 2.044776839256287, Validation Loss: 2.0129565095901487\n",
      "Epoch: 294/300, Train Loss: 2.067017903327942, Validation Loss: 2.010569815635681\n",
      "Epoch: 295/300, Train Loss: 2.0531767416000366, Validation Loss: 2.0008793830871583\n",
      "Epoch: 296/300, Train Loss: 2.0603892269134523, Validation Loss: 1.976509575843811\n",
      "Epoch: 297/300, Train Loss: 2.079596605300903, Validation Loss: 2.0052252292633055\n",
      "Epoch: 298/300, Train Loss: 2.055687314987183, Validation Loss: 1.9991713428497315\n",
      "Epoch: 299/300, Train Loss: 2.053237593650818, Validation Loss: 1.9935777187347412\n",
      "Epoch: 300/300, Train Loss: 2.0519885449409485, Validation Loss: 2.0351191186904907\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "\n",
    "\n",
    "for epoch in range(epoch_num):\n",
    "\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for inputs, outputs in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        inputs = inputs.to(dtype).to(device)\n",
    "        outputs = outputs.to(dtype).to(device)\n",
    "\n",
    "        predictions = model(inputs)\n",
    "        loss = criterion(predictions.squeeze(1), outputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # update training loss\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, outputs in val_loader:\n",
    "\n",
    "            inputs = inputs.to(dtype).to(device)\n",
    "            outputs = outputs.to(dtype).to(device)\n",
    "\n",
    "            predictions = model(inputs)\n",
    "            loss = criterion(predictions.squeeze(1), outputs)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    print(f\"Epoch: {epoch+1}/{epoch_num}, Train Loss: {train_loss/len(train_loader)}, Validation Loss: {val_loss/len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.67 [2, 128, 128, 1] 600epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0000, -0.5760],\n",
      "        [ 0.7370,  0.8590],\n",
      "        [ 0.8790, -0.6360],\n",
      "        [ 0.9800, -0.1920],\n",
      "        [-0.0909, -0.7780],\n",
      "        [-0.5150, -0.8990],\n",
      "        [-0.8180, -0.9800],\n",
      "        [-0.6360,  0.2930],\n",
      "        [ 0.3940,  0.9390],\n",
      "        [-0.5560, -0.6970],\n",
      "        [ 0.6570, -0.2320],\n",
      "        [-0.2120,  0.6970],\n",
      "        [ 0.1520,  0.1110],\n",
      "        [ 0.2120, -0.6770],\n",
      "        [ 0.3330,  0.7980],\n",
      "        [ 0.2730, -0.8180],\n",
      "        [-0.2530, -0.0909],\n",
      "        [-0.3940,  0.7580],\n",
      "        [-0.0707,  0.8180],\n",
      "        [ 0.3130,  0.0909],\n",
      "        [ 0.4750, -0.6770],\n",
      "        [-0.0707,  0.5350],\n",
      "        [-0.0303, -0.5350],\n",
      "        [-0.5150,  0.5560],\n",
      "        [-0.4140,  0.5350],\n",
      "        [ 0.1310,  0.3130],\n",
      "        [-0.7980, -0.6360],\n",
      "        [ 0.9600,  0.9600],\n",
      "        [ 0.6570, -0.7980],\n",
      "        [ 0.1520,  0.8380],\n",
      "        [ 0.8990,  0.7370],\n",
      "        [ 0.1720, -0.1310],\n",
      "        [-0.6160,  0.5760],\n",
      "        [-0.1720,  0.6160],\n",
      "        [ 0.9800,  0.8180],\n",
      "        [ 0.1720, -0.8790],\n",
      "        [ 0.6360,  0.3130],\n",
      "        [ 0.4950, -1.0000],\n",
      "        [ 0.5350,  0.7780],\n",
      "        [ 0.1720,  0.1720],\n",
      "        [-0.7780, -0.9600],\n",
      "        [ 0.5560,  0.1520],\n",
      "        [-0.3330,  0.3130],\n",
      "        [ 0.1110,  0.3940],\n",
      "        [ 0.0707,  0.9390],\n",
      "        [-0.3330,  0.8590],\n",
      "        [-0.4950, -0.5150],\n",
      "        [-0.9190, -0.6970],\n",
      "        [ 0.9600,  0.7780],\n",
      "        [ 0.3330, -0.7370],\n",
      "        [-0.9800, -0.3330],\n",
      "        [-0.1520,  0.8990],\n",
      "        [-0.6570,  0.0707],\n",
      "        [-0.7370,  0.7370],\n",
      "        [-0.6770,  0.2530],\n",
      "        [ 0.5760,  0.2530],\n",
      "        [-0.7170,  1.0000],\n",
      "        [ 0.7170, -0.4750],\n",
      "        [-0.1520, -0.1310],\n",
      "        [ 0.1520, -0.5960],\n",
      "        [-0.3130, -0.0101],\n",
      "        [-0.5960, -0.6160],\n",
      "        [ 0.4750,  0.9600],\n",
      "        [ 0.6160,  0.5960]], dtype=torch.float64)\n",
      "tensor([1.5300, 1.4500, 1.3400, 0.8520, 0.7280, 0.8000, 0.9720, 1.1300, 1.0800,\n",
      "        0.6580, 0.8600, 1.0400, 1.1800, 1.0600, 0.4960, 0.7590, 1.4700, 0.6180,\n",
      "        0.7210, 1.2500, 0.5980, 1.5800, 1.8000, 0.8230, 1.2800, 1.2900, 0.9800,\n",
      "        0.5910, 0.9440, 0.5020, 1.3900, 1.1500, 0.6260, 1.2100, 0.7330, 0.4550,\n",
      "        0.8760, 1.3100, 0.4900, 1.0400, 1.0800, 1.5400, 1.6900, 1.5500, 0.4040,\n",
      "        0.2390, 0.8150, 1.5000, 1.1300, 0.5450, 1.3100, 0.5780, 1.4100, 0.9030,\n",
      "        0.9900, 1.3700, 1.1600, 0.3190, 1.0500, 1.5100, 1.6100, 0.5140, 1.1300,\n",
      "        0.3930], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "a = iter(train_loader)\n",
    "b,c = next(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
