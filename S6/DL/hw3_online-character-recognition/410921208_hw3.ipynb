{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "from PIL import Image\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Serial No.</th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "      <th>x3</th>\n",
       "      <th>y3</th>\n",
       "      <th>x4</th>\n",
       "      <th>y4</th>\n",
       "      <th>x5</th>\n",
       "      <th>y5</th>\n",
       "      <th>x6</th>\n",
       "      <th>y6</th>\n",
       "      <th>x7</th>\n",
       "      <th>y7</th>\n",
       "      <th>x8</th>\n",
       "      <th>y8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>100</td>\n",
       "      <td>27</td>\n",
       "      <td>81</td>\n",
       "      <td>57</td>\n",
       "      <td>37</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>56</td>\n",
       "      <td>53</td>\n",
       "      <td>100</td>\n",
       "      <td>90</td>\n",
       "      <td>40</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "      <td>27</td>\n",
       "      <td>100</td>\n",
       "      <td>42</td>\n",
       "      <td>75</td>\n",
       "      <td>29</td>\n",
       "      <td>45</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>31</td>\n",
       "      <td>68</td>\n",
       "      <td>72</td>\n",
       "      <td>90</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>76</td>\n",
       "      <td>75</td>\n",
       "      <td>50</td>\n",
       "      <td>51</td>\n",
       "      <td>28</td>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>7</td>\n",
       "      <td>92</td>\n",
       "      <td>5</td>\n",
       "      <td>68</td>\n",
       "      <td>19</td>\n",
       "      <td>45</td>\n",
       "      <td>86</td>\n",
       "      <td>34</td>\n",
       "      <td>100</td>\n",
       "      <td>45</td>\n",
       "      <td>74</td>\n",
       "      <td>23</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>49</td>\n",
       "      <td>83</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>81</td>\n",
       "      <td>80</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>33</td>\n",
       "      <td>20</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7489</th>\n",
       "      <td>7490</td>\n",
       "      <td>0</td>\n",
       "      <td>82</td>\n",
       "      <td>9</td>\n",
       "      <td>59</td>\n",
       "      <td>56</td>\n",
       "      <td>34</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>67</td>\n",
       "      <td>42</td>\n",
       "      <td>96</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7490</th>\n",
       "      <td>7491</td>\n",
       "      <td>49</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>24</td>\n",
       "      <td>56</td>\n",
       "      <td>100</td>\n",
       "      <td>65</td>\n",
       "      <td>86</td>\n",
       "      <td>85</td>\n",
       "      <td>44</td>\n",
       "      <td>77</td>\n",
       "      <td>21</td>\n",
       "      <td>38</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7491</th>\n",
       "      <td>7492</td>\n",
       "      <td>100</td>\n",
       "      <td>98</td>\n",
       "      <td>60</td>\n",
       "      <td>100</td>\n",
       "      <td>24</td>\n",
       "      <td>87</td>\n",
       "      <td>3</td>\n",
       "      <td>58</td>\n",
       "      <td>35</td>\n",
       "      <td>51</td>\n",
       "      <td>58</td>\n",
       "      <td>26</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7492</th>\n",
       "      <td>7493</td>\n",
       "      <td>59</td>\n",
       "      <td>65</td>\n",
       "      <td>91</td>\n",
       "      <td>100</td>\n",
       "      <td>84</td>\n",
       "      <td>96</td>\n",
       "      <td>72</td>\n",
       "      <td>50</td>\n",
       "      <td>51</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7493</th>\n",
       "      <td>7494</td>\n",
       "      <td>0</td>\n",
       "      <td>78</td>\n",
       "      <td>29</td>\n",
       "      <td>100</td>\n",
       "      <td>94</td>\n",
       "      <td>86</td>\n",
       "      <td>70</td>\n",
       "      <td>48</td>\n",
       "      <td>42</td>\n",
       "      <td>11</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>36</td>\n",
       "      <td>100</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7494 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Serial No.   x1   y1  x2   y2   x3   y3   x4   y4  x5  y5   x6  y6   x7   \n",
       "0              1   47  100  27   81   57   37   26    0   0  23   56  53  100  \\\n",
       "1              2    0   89  27  100   42   75   29   45  15  15   37   0   69   \n",
       "2              3    0   57  31   68   72   90  100  100  76  75   50  51   28   \n",
       "3              4    0  100   7   92    5   68   19   45  86  34  100  45   74   \n",
       "4              5    0   67  49   83  100  100   81   80  60  60   40  40   33   \n",
       "...          ...  ...  ...  ..  ...  ...  ...  ...  ...  ..  ..  ...  ..  ...   \n",
       "7489        7490    0   82   9   59   56   34   41    0  10  30    3  67   42   \n",
       "7490        7491   49  100   0   70   24   56  100   65  86  85   44  77   21   \n",
       "7491        7492  100   98  60  100   24   87    3   58  35  51   58  26   36   \n",
       "7492        7493   59   65  91  100   84   96   72   50  51   8    0   0   45   \n",
       "7493        7494    0   78  29  100   94   86   70   48  42  11   32   0   25   \n",
       "\n",
       "      y7   x8   y8  \n",
       "0     90   40   98  \n",
       "1      2  100    6  \n",
       "2     25   16    0  \n",
       "3     23   67    0  \n",
       "4     20   47    0  \n",
       "...   ..  ...  ...  \n",
       "7489  96  100  100  \n",
       "7490  38    6    0  \n",
       "7491   0    0    5  \n",
       "7492   1  100    0  \n",
       "7493  36  100   40  \n",
       "\n",
       "[7494 rows x 17 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/train_in.csv')\n",
    "df_2 = pd.read_csv('data/train_out.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Serial No.</th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "      <th>x3</th>\n",
       "      <th>y3</th>\n",
       "      <th>x4</th>\n",
       "      <th>y4</th>\n",
       "      <th>x5</th>\n",
       "      <th>y5</th>\n",
       "      <th>x6</th>\n",
       "      <th>y6</th>\n",
       "      <th>x7</th>\n",
       "      <th>y7</th>\n",
       "      <th>x8</th>\n",
       "      <th>y8</th>\n",
       "      <th>Serial No.</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>100</td>\n",
       "      <td>27</td>\n",
       "      <td>81</td>\n",
       "      <td>57</td>\n",
       "      <td>37</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>56</td>\n",
       "      <td>53</td>\n",
       "      <td>100</td>\n",
       "      <td>90</td>\n",
       "      <td>40</td>\n",
       "      <td>98</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "      <td>27</td>\n",
       "      <td>100</td>\n",
       "      <td>42</td>\n",
       "      <td>75</td>\n",
       "      <td>29</td>\n",
       "      <td>45</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>31</td>\n",
       "      <td>68</td>\n",
       "      <td>72</td>\n",
       "      <td>90</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>76</td>\n",
       "      <td>75</td>\n",
       "      <td>50</td>\n",
       "      <td>51</td>\n",
       "      <td>28</td>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>7</td>\n",
       "      <td>92</td>\n",
       "      <td>5</td>\n",
       "      <td>68</td>\n",
       "      <td>19</td>\n",
       "      <td>45</td>\n",
       "      <td>86</td>\n",
       "      <td>34</td>\n",
       "      <td>100</td>\n",
       "      <td>45</td>\n",
       "      <td>74</td>\n",
       "      <td>23</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>49</td>\n",
       "      <td>83</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>81</td>\n",
       "      <td>80</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>33</td>\n",
       "      <td>20</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7489</th>\n",
       "      <td>7490</td>\n",
       "      <td>0</td>\n",
       "      <td>82</td>\n",
       "      <td>9</td>\n",
       "      <td>59</td>\n",
       "      <td>56</td>\n",
       "      <td>34</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>67</td>\n",
       "      <td>42</td>\n",
       "      <td>96</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7490</th>\n",
       "      <td>7491</td>\n",
       "      <td>49</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>24</td>\n",
       "      <td>56</td>\n",
       "      <td>100</td>\n",
       "      <td>65</td>\n",
       "      <td>86</td>\n",
       "      <td>85</td>\n",
       "      <td>44</td>\n",
       "      <td>77</td>\n",
       "      <td>21</td>\n",
       "      <td>38</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7491</th>\n",
       "      <td>7492</td>\n",
       "      <td>100</td>\n",
       "      <td>98</td>\n",
       "      <td>60</td>\n",
       "      <td>100</td>\n",
       "      <td>24</td>\n",
       "      <td>87</td>\n",
       "      <td>3</td>\n",
       "      <td>58</td>\n",
       "      <td>35</td>\n",
       "      <td>51</td>\n",
       "      <td>58</td>\n",
       "      <td>26</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7492</th>\n",
       "      <td>7493</td>\n",
       "      <td>59</td>\n",
       "      <td>65</td>\n",
       "      <td>91</td>\n",
       "      <td>100</td>\n",
       "      <td>84</td>\n",
       "      <td>96</td>\n",
       "      <td>72</td>\n",
       "      <td>50</td>\n",
       "      <td>51</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7493</th>\n",
       "      <td>7494</td>\n",
       "      <td>0</td>\n",
       "      <td>78</td>\n",
       "      <td>29</td>\n",
       "      <td>100</td>\n",
       "      <td>94</td>\n",
       "      <td>86</td>\n",
       "      <td>70</td>\n",
       "      <td>48</td>\n",
       "      <td>42</td>\n",
       "      <td>11</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>36</td>\n",
       "      <td>100</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7494 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Serial No.   x1   y1  x2   y2   x3   y3   x4   y4  x5  y5   x6  y6   x7   \n",
       "0              1   47  100  27   81   57   37   26    0   0  23   56  53  100  \\\n",
       "1              2    0   89  27  100   42   75   29   45  15  15   37   0   69   \n",
       "2              3    0   57  31   68   72   90  100  100  76  75   50  51   28   \n",
       "3              4    0  100   7   92    5   68   19   45  86  34  100  45   74   \n",
       "4              5    0   67  49   83  100  100   81   80  60  60   40  40   33   \n",
       "...          ...  ...  ...  ..  ...  ...  ...  ...  ...  ..  ..  ...  ..  ...   \n",
       "7489        7490    0   82   9   59   56   34   41    0  10  30    3  67   42   \n",
       "7490        7491   49  100   0   70   24   56  100   65  86  85   44  77   21   \n",
       "7491        7492  100   98  60  100   24   87    3   58  35  51   58  26   36   \n",
       "7492        7493   59   65  91  100   84   96   72   50  51   8    0   0   45   \n",
       "7493        7494    0   78  29  100   94   86   70   48  42  11   32   0   25   \n",
       "\n",
       "      y7   x8   y8  Serial No.  Label  \n",
       "0     90   40   98         NaN      8  \n",
       "1      2  100    6         NaN      2  \n",
       "2     25   16    0         NaN      1  \n",
       "3     23   67    0         NaN      4  \n",
       "4     20   47    0         NaN      1  \n",
       "...   ..  ...  ...         ...    ...  \n",
       "7489  96  100  100         NaN      5  \n",
       "7490  38    6    0         NaN      4  \n",
       "7491   0    0    5         NaN      5  \n",
       "7492   1  100    0         NaN      1  \n",
       "7493  36  100   40         NaN      7  \n",
       "\n",
       "[7494 rows x 19 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = pd.concat([df, df_2], axis=1)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "      <th>x3</th>\n",
       "      <th>y3</th>\n",
       "      <th>x4</th>\n",
       "      <th>y4</th>\n",
       "      <th>x5</th>\n",
       "      <th>y5</th>\n",
       "      <th>x6</th>\n",
       "      <th>y6</th>\n",
       "      <th>x7</th>\n",
       "      <th>y7</th>\n",
       "      <th>x8</th>\n",
       "      <th>y8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47</td>\n",
       "      <td>100</td>\n",
       "      <td>27</td>\n",
       "      <td>81</td>\n",
       "      <td>57</td>\n",
       "      <td>37</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>56</td>\n",
       "      <td>53</td>\n",
       "      <td>100</td>\n",
       "      <td>90</td>\n",
       "      <td>40</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "      <td>27</td>\n",
       "      <td>100</td>\n",
       "      <td>42</td>\n",
       "      <td>75</td>\n",
       "      <td>29</td>\n",
       "      <td>45</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>31</td>\n",
       "      <td>68</td>\n",
       "      <td>72</td>\n",
       "      <td>90</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>76</td>\n",
       "      <td>75</td>\n",
       "      <td>50</td>\n",
       "      <td>51</td>\n",
       "      <td>28</td>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>7</td>\n",
       "      <td>92</td>\n",
       "      <td>5</td>\n",
       "      <td>68</td>\n",
       "      <td>19</td>\n",
       "      <td>45</td>\n",
       "      <td>86</td>\n",
       "      <td>34</td>\n",
       "      <td>100</td>\n",
       "      <td>45</td>\n",
       "      <td>74</td>\n",
       "      <td>23</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>49</td>\n",
       "      <td>83</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>81</td>\n",
       "      <td>80</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>33</td>\n",
       "      <td>20</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7489</th>\n",
       "      <td>0</td>\n",
       "      <td>82</td>\n",
       "      <td>9</td>\n",
       "      <td>59</td>\n",
       "      <td>56</td>\n",
       "      <td>34</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>67</td>\n",
       "      <td>42</td>\n",
       "      <td>96</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7490</th>\n",
       "      <td>49</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>24</td>\n",
       "      <td>56</td>\n",
       "      <td>100</td>\n",
       "      <td>65</td>\n",
       "      <td>86</td>\n",
       "      <td>85</td>\n",
       "      <td>44</td>\n",
       "      <td>77</td>\n",
       "      <td>21</td>\n",
       "      <td>38</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7491</th>\n",
       "      <td>100</td>\n",
       "      <td>98</td>\n",
       "      <td>60</td>\n",
       "      <td>100</td>\n",
       "      <td>24</td>\n",
       "      <td>87</td>\n",
       "      <td>3</td>\n",
       "      <td>58</td>\n",
       "      <td>35</td>\n",
       "      <td>51</td>\n",
       "      <td>58</td>\n",
       "      <td>26</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7492</th>\n",
       "      <td>59</td>\n",
       "      <td>65</td>\n",
       "      <td>91</td>\n",
       "      <td>100</td>\n",
       "      <td>84</td>\n",
       "      <td>96</td>\n",
       "      <td>72</td>\n",
       "      <td>50</td>\n",
       "      <td>51</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7493</th>\n",
       "      <td>0</td>\n",
       "      <td>78</td>\n",
       "      <td>29</td>\n",
       "      <td>100</td>\n",
       "      <td>94</td>\n",
       "      <td>86</td>\n",
       "      <td>70</td>\n",
       "      <td>48</td>\n",
       "      <td>42</td>\n",
       "      <td>11</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>36</td>\n",
       "      <td>100</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7494 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       x1   y1  x2   y2   x3   y3   x4   y4  x5  y5   x6  y6   x7  y7   x8   \n",
       "0      47  100  27   81   57   37   26    0   0  23   56  53  100  90   40  \\\n",
       "1       0   89  27  100   42   75   29   45  15  15   37   0   69   2  100   \n",
       "2       0   57  31   68   72   90  100  100  76  75   50  51   28  25   16   \n",
       "3       0  100   7   92    5   68   19   45  86  34  100  45   74  23   67   \n",
       "4       0   67  49   83  100  100   81   80  60  60   40  40   33  20   47   \n",
       "...   ...  ...  ..  ...  ...  ...  ...  ...  ..  ..  ...  ..  ...  ..  ...   \n",
       "7489    0   82   9   59   56   34   41    0  10  30    3  67   42  96  100   \n",
       "7490   49  100   0   70   24   56  100   65  86  85   44  77   21  38    6   \n",
       "7491  100   98  60  100   24   87    3   58  35  51   58  26   36   0    0   \n",
       "7492   59   65  91  100   84   96   72   50  51   8    0   0   45   1  100   \n",
       "7493    0   78  29  100   94   86   70   48  42  11   32   0   25  36  100   \n",
       "\n",
       "       y8  \n",
       "0      98  \n",
       "1       6  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  \n",
       "...   ...  \n",
       "7489  100  \n",
       "7490    0  \n",
       "7491    5  \n",
       "7492    0  \n",
       "7493   40  \n",
       "\n",
       "[7494 rows x 16 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(['Serial No.'], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7494, 8, 2)\n"
     ]
    }
   ],
   "source": [
    "# 將資料轉換為所需的形狀\n",
    "reshaped_data = df.values.reshape(-1, 8, 2)\n",
    "\n",
    "print(reshaped_data.shape)  # 輸出應該是(100, 8, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 47, 100],\n",
       "        [ 27,  81],\n",
       "        [ 57,  37],\n",
       "        ...,\n",
       "        [ 56,  53],\n",
       "        [100,  90],\n",
       "        [ 40,  98]],\n",
       "\n",
       "       [[  0,  89],\n",
       "        [ 27, 100],\n",
       "        [ 42,  75],\n",
       "        ...,\n",
       "        [ 37,   0],\n",
       "        [ 69,   2],\n",
       "        [100,   6]],\n",
       "\n",
       "       [[  0,  57],\n",
       "        [ 31,  68],\n",
       "        [ 72,  90],\n",
       "        ...,\n",
       "        [ 50,  51],\n",
       "        [ 28,  25],\n",
       "        [ 16,   0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[100,  98],\n",
       "        [ 60, 100],\n",
       "        [ 24,  87],\n",
       "        ...,\n",
       "        [ 58,  26],\n",
       "        [ 36,   0],\n",
       "        [  0,   5]],\n",
       "\n",
       "       [[ 59,  65],\n",
       "        [ 91, 100],\n",
       "        [ 84,  96],\n",
       "        ...,\n",
       "        [  0,   0],\n",
       "        [ 45,   1],\n",
       "        [100,   0]],\n",
       "\n",
       "       [[  0,  78],\n",
       "        [ 29, 100],\n",
       "        [ 94,  86],\n",
       "        ...,\n",
       "        [ 32,   0],\n",
       "        [ 25,  36],\n",
       "        [100,  40]]], dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x1     47\n",
       "y1    100\n",
       "x2     27\n",
       "y2     81\n",
       "x3     57\n",
       "y3     37\n",
       "x4     26\n",
       "y4      0\n",
       "x5      0\n",
       "y5     23\n",
       "x6     56\n",
       "y6     53\n",
       "x7    100\n",
       "y7     90\n",
       "x8     40\n",
       "y8     98\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = str(Path(fr\"./runs/exp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 8\n",
    "batch_size = 16\n",
    "epoch_num = 200\n",
    "\n",
    "LR = 1e-3\n",
    "EPS = 1e-9\n",
    "LR_STEP = 20\n",
    "LR_GAMMA = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.data = pd.read_csv('data/train_in.csv').drop(['Serial No.'], axis=1)  # 從csv文件中讀取數據\n",
    "        self.label = pd.read_csv('data/train_out.csv').drop(['Serial No.'], axis=1) # 從csv文件中讀取數據\n",
    "        self.reshaped_data = torch.from_numpy(self.data.values.reshape(-1, 8, 2))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)  # 返回數據集的大小\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.reshaped_data[idx]\n",
    "        label = self.label.iloc[idx]\n",
    "        return data, label  # 返回圖像和標籤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7494\n"
     ]
    }
   ],
   "source": [
    "# 創建一個自定義的圖像數據集對象\n",
    "dataset = CustomImageDataset()\n",
    "\n",
    "# 計算訓練集的大小，通常是數據集的80%\n",
    "train_size = int(0.8 * len(dataset))\n",
    "\n",
    "# 驗證集的大小就是剩餘的部分\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# 使用random_split函數隨機分割數據集為訓練集和驗證集\n",
    "train_dataset, valid_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# 創建一個數據加載器的字典，對訓練集和驗證集分別進行加載\n",
    "# shuffle參數設為True表示在每個訓練時期開始時，數據加載器會重新洗牌數據\n",
    "dataloaders = {x: DataLoader(y, batch_size=batch_size, shuffle=z) \n",
    "               for x, y, z in zip(['train', 'valid'], [train_dataset, valid_dataset], [True, False])}\n",
    "\n",
    "# 打印整個數據集的大小\n",
    "print(len(dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50\n",
    "\n",
    "model_name =  MusicScaleNet\n",
    "\n",
    "# 設定不同網路的參數\n",
    "model_kwargs = {\n",
    "    'block': 'InvertedResidualBlock', \n",
    "    'block_config': [[2, 6, 64, 1], [2, 6, 64, 2], [2, 6, 128, 2], [2, 6, 256, 2], [2, 6, 512, 2]]\n",
    "}\n",
    "model_kwargs = {\n",
    "    'block': 'ResBlock', \n",
    "    'block_config': [[2, 64, 1], [2, 64, 2], [2, 128, 2], [2, 256, 2], [2, 512, 2]]\n",
    "}\n",
    "\n",
    "# # 初始化模型\n",
    "# model = resnet50()\n",
    "\n",
    "\n",
    "model = MusicScaleNet(**model_kwargs)\n",
    "\n",
    "# 選擇優化器，使用AdamW\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, eps=EPS)\n",
    "# 使用SGD作為優化器\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "\n",
    "# 使用ReduceLROnPlateau學習率調度器，當某指標停止改進時，減少學習率\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
    "                                                          mode='min', \n",
    "                                                          factor=0.5, \n",
    "                                                          patience=30, \n",
    "                                                          verbose=True, \n",
    "                                                          threshold=0.0001, \n",
    "                                                          threshold_mode='rel', \n",
    "                                                          cooldown=50, \n",
    "                                                          min_lr=0, \n",
    "                                                          eps=EPS)\n",
    "\n",
    "# 使用ReduceLROnPlateau學習率調度器，當epoch計數器到一定值後會更先學習率\n",
    "# lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=LR_STEP, gamma=LR_GAMMA)\n",
    "\n",
    "# 選擇損失函數，這裡我們使用交叉熵損失函數\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: module Bottleneck is treated as a zero-op.\n",
      "Warning: module ResNet is treated as a zero-op.\n",
      "ResNet(\n",
      "  25.56 M, 100.000% Params, 17.39 GMac, 100.000% MACs, \n",
      "  (conv1): Conv2d(9.41 k, 0.037% Params, 481.88 MMac, 2.771% MACs, 3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(128, 0.001% Params, 6.56 MMac, 0.038% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(0, 0.000% Params, 3.28 MMac, 0.019% MACs, inplace=True)\n",
      "  (maxpool): MaxPool2d(0, 0.000% Params, 3.28 MMac, 0.019% MACs, kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    215.81 k, 0.844% Params, 2.79 GMac, 16.054% MACs, \n",
      "    (0): Bottleneck(\n",
      "      75.01 k, 0.293% Params, 970.3 MMac, 5.579% MACs, \n",
      "      (conv1): Conv2d(4.1 k, 0.016% Params, 52.72 MMac, 0.303% MACs, 64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, 0.001% Params, 1.65 MMac, 0.009% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(36.86 k, 0.144% Params, 474.44 MMac, 2.728% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, 0.001% Params, 1.65 MMac, 0.009% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(16.38 k, 0.064% Params, 210.86 MMac, 1.212% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, 0.002% Params, 6.59 MMac, 0.038% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0, 0.000% Params, 4.94 MMac, 0.028% MACs, inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        16.9 k, 0.066% Params, 217.45 MMac, 1.250% MACs, \n",
      "        (0): Conv2d(16.38 k, 0.064% Params, 210.86 MMac, 1.212% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(512, 0.002% Params, 6.59 MMac, 0.038% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      70.4 k, 0.275% Params, 910.99 MMac, 5.238% MACs, \n",
      "      (conv1): Conv2d(16.38 k, 0.064% Params, 210.86 MMac, 1.212% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, 0.001% Params, 1.65 MMac, 0.009% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(36.86 k, 0.144% Params, 474.44 MMac, 2.728% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, 0.001% Params, 1.65 MMac, 0.009% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(16.38 k, 0.064% Params, 210.86 MMac, 1.212% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, 0.002% Params, 6.59 MMac, 0.038% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0, 0.000% Params, 4.94 MMac, 0.028% MACs, inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      70.4 k, 0.275% Params, 910.99 MMac, 5.238% MACs, \n",
      "      (conv1): Conv2d(16.38 k, 0.064% Params, 210.86 MMac, 1.212% MACs, 256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, 0.001% Params, 1.65 MMac, 0.009% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(36.86 k, 0.144% Params, 474.44 MMac, 2.728% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, 0.001% Params, 1.65 MMac, 0.009% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(16.38 k, 0.064% Params, 210.86 MMac, 1.212% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, 0.002% Params, 6.59 MMac, 0.038% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0, 0.000% Params, 4.94 MMac, 0.028% MACs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    1.22 M, 4.772% Params, 4.29 GMac, 24.680% MACs, \n",
      "    (0): Bottleneck(\n",
      "      379.39 k, 1.484% Params, 1.55 GMac, 8.937% MACs, \n",
      "      (conv1): Conv2d(32.77 k, 0.128% Params, 421.72 MMac, 2.425% MACs, 256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, 0.001% Params, 3.29 MMac, 0.019% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(147.46 k, 0.577% Params, 479.23 MMac, 2.755% MACs, 128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, 0.001% Params, 832.0 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(65.54 k, 0.256% Params, 212.99 MMac, 1.225% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 3.33 MMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0, 0.000% Params, 3.73 MMac, 0.021% MACs, inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        132.1 k, 0.517% Params, 429.31 MMac, 2.468% MACs, \n",
      "        (0): Conv2d(131.07 k, 0.513% Params, 425.98 MMac, 2.449% MACs, 256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1.02 k, 0.004% Params, 3.33 MMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      280.06 k, 1.096% Params, 912.7 MMac, 5.248% MACs, \n",
      "      (conv1): Conv2d(65.54 k, 0.256% Params, 212.99 MMac, 1.225% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, 0.001% Params, 832.0 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(147.46 k, 0.577% Params, 479.23 MMac, 2.755% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, 0.001% Params, 832.0 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(65.54 k, 0.256% Params, 212.99 MMac, 1.225% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 3.33 MMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0, 0.000% Params, 2.5 MMac, 0.014% MACs, inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      280.06 k, 1.096% Params, 912.7 MMac, 5.248% MACs, \n",
      "      (conv1): Conv2d(65.54 k, 0.256% Params, 212.99 MMac, 1.225% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, 0.001% Params, 832.0 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(147.46 k, 0.577% Params, 479.23 MMac, 2.755% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, 0.001% Params, 832.0 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(65.54 k, 0.256% Params, 212.99 MMac, 1.225% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 3.33 MMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0, 0.000% Params, 2.5 MMac, 0.014% MACs, inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      280.06 k, 1.096% Params, 912.7 MMac, 5.248% MACs, \n",
      "      (conv1): Conv2d(65.54 k, 0.256% Params, 212.99 MMac, 1.225% MACs, 512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, 0.001% Params, 832.0 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(147.46 k, 0.577% Params, 479.23 MMac, 2.755% MACs, 128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, 0.001% Params, 832.0 KMac, 0.005% MACs, 128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(65.54 k, 0.256% Params, 212.99 MMac, 1.225% MACs, 128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1.02 k, 0.004% Params, 3.33 MMac, 0.019% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0, 0.000% Params, 2.5 MMac, 0.014% MACs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    7.1 M, 27.775% Params, 6.18 GMac, 35.552% MACs, \n",
      "    (0): Bottleneck(\n",
      "      1.51 M, 5.918% Params, 1.57 GMac, 9.020% MACs, \n",
      "      (conv1): Conv2d(131.07 k, 0.513% Params, 425.98 MMac, 2.449% MACs, 512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, 0.002% Params, 1.66 MMac, 0.010% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(589.82 k, 2.308% Params, 486.6 MMac, 2.798% MACs, 256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, 0.002% Params, 422.4 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(262.14 k, 1.026% Params, 216.27 MMac, 1.243% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 1.69 MMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0, 0.000% Params, 1.89 MMac, 0.011% MACs, inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        526.34 k, 2.059% Params, 434.23 MMac, 2.497% MACs, \n",
      "        (0): Conv2d(524.29 k, 2.051% Params, 432.54 MMac, 2.487% MACs, 512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2.05 k, 0.008% Params, 1.69 MMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      1.12 M, 4.371% Params, 922.94 MMac, 5.307% MACs, \n",
      "      (conv1): Conv2d(262.14 k, 1.026% Params, 216.27 MMac, 1.243% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, 0.002% Params, 422.4 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(589.82 k, 2.308% Params, 486.6 MMac, 2.798% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, 0.002% Params, 422.4 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(262.14 k, 1.026% Params, 216.27 MMac, 1.243% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 1.69 MMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0, 0.000% Params, 1.27 MMac, 0.007% MACs, inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      1.12 M, 4.371% Params, 922.94 MMac, 5.307% MACs, \n",
      "      (conv1): Conv2d(262.14 k, 1.026% Params, 216.27 MMac, 1.243% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, 0.002% Params, 422.4 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(589.82 k, 2.308% Params, 486.6 MMac, 2.798% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, 0.002% Params, 422.4 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(262.14 k, 1.026% Params, 216.27 MMac, 1.243% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 1.69 MMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0, 0.000% Params, 1.27 MMac, 0.007% MACs, inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      1.12 M, 4.371% Params, 922.94 MMac, 5.307% MACs, \n",
      "      (conv1): Conv2d(262.14 k, 1.026% Params, 216.27 MMac, 1.243% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, 0.002% Params, 422.4 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(589.82 k, 2.308% Params, 486.6 MMac, 2.798% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, 0.002% Params, 422.4 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(262.14 k, 1.026% Params, 216.27 MMac, 1.243% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 1.69 MMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0, 0.000% Params, 1.27 MMac, 0.007% MACs, inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      1.12 M, 4.371% Params, 922.94 MMac, 5.307% MACs, \n",
      "      (conv1): Conv2d(262.14 k, 1.026% Params, 216.27 MMac, 1.243% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, 0.002% Params, 422.4 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(589.82 k, 2.308% Params, 486.6 MMac, 2.798% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, 0.002% Params, 422.4 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(262.14 k, 1.026% Params, 216.27 MMac, 1.243% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 1.69 MMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0, 0.000% Params, 1.27 MMac, 0.007% MACs, inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      1.12 M, 4.371% Params, 922.94 MMac, 5.307% MACs, \n",
      "      (conv1): Conv2d(262.14 k, 1.026% Params, 216.27 MMac, 1.243% MACs, 1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, 0.002% Params, 422.4 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(589.82 k, 2.308% Params, 486.6 MMac, 2.798% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, 0.002% Params, 422.4 KMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(262.14 k, 1.026% Params, 216.27 MMac, 1.243% MACs, 256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2.05 k, 0.008% Params, 1.69 MMac, 0.010% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0, 0.000% Params, 1.27 MMac, 0.007% MACs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    14.96 M, 58.554% Params, 3.63 GMac, 20.853% MACs, \n",
      "    (0): Bottleneck(\n",
      "      6.04 M, 23.632% Params, 1.65 GMac, 9.504% MACs, \n",
      "      (conv1): Conv2d(524.29 k, 2.051% Params, 432.54 MMac, 2.487% MACs, 1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 844.8 KMac, 0.005% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(2.36 M, 9.231% Params, 521.4 MMac, 2.998% MACs, 512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 226.3 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(1.05 M, 4.103% Params, 231.74 MMac, 1.332% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 905.22 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0, 0.000% Params, 988.16 KMac, 0.006% MACs, inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        2.1 M, 8.222% Params, 464.38 MMac, 2.670% MACs, \n",
      "        (0): Conv2d(2.1 M, 8.206% Params, 463.47 MMac, 2.665% MACs, 1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(4.1 k, 0.016% Params, 905.22 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      4.46 M, 17.461% Params, 986.91 MMac, 5.674% MACs, \n",
      "      (conv1): Conv2d(1.05 M, 4.103% Params, 231.74 MMac, 1.332% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 226.3 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(2.36 M, 9.231% Params, 521.4 MMac, 2.998% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 226.3 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(1.05 M, 4.103% Params, 231.74 MMac, 1.332% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 905.22 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0, 0.000% Params, 678.91 KMac, 0.004% MACs, inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      4.46 M, 17.461% Params, 986.91 MMac, 5.674% MACs, \n",
      "      (conv1): Conv2d(1.05 M, 4.103% Params, 231.74 MMac, 1.332% MACs, 2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(1.02 k, 0.004% Params, 226.3 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(2.36 M, 9.231% Params, 521.4 MMac, 2.998% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(1.02 k, 0.004% Params, 226.3 KMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(1.05 M, 4.103% Params, 231.74 MMac, 1.332% MACs, 512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(4.1 k, 0.016% Params, 905.22 KMac, 0.005% MACs, 2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0, 0.000% Params, 678.91 KMac, 0.004% MACs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(0, 0.000% Params, 452.61 KMac, 0.003% MACs, output_size=(1, 1))\n",
      "  (fc): Linear(2.05 M, 8.017% Params, 2.05 MMac, 0.012% MACs, in_features=2048, out_features=1000, bias=True)\n",
      ")\n",
      "Computational complexity:       17.39 GMac\n",
      "Number of parameters:           25.56 M \n"
     ]
    }
   ],
   "source": [
    "# 導入ptflops庫，用於獲取模型的複雜度信息\n",
    "from ptflops import get_model_complexity_info\n",
    "\n",
    "# 指定使用第一塊GPU（索引為0）\n",
    "with torch.cuda.device(0):\n",
    "    # 獲取模型的複雜度信息，包括MACs和參數數量\n",
    "    macs, params = get_model_complexity_info(model, (3, 520, 394), as_strings=True,\n",
    "                                           print_per_layer_stat=True, verbose=True)\n",
    "    # 打印模型的計算複雜度（MACs）\n",
    "    print('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
    "    # 打印模型的參數數量\n",
    "    print('{:<30}  {:<8}'.format('Number of parameters: ', params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = {\n",
    "    'epoch': 0,\n",
    "    'model_name': model_name,\n",
    "    'model_args': model_kwargs,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'lr_scheduler_state_dict': lr_scheduler.state_dict(),\n",
    "    'criterion': criterion,\n",
    "    'history': {\n",
    "        'train_loss': [], \n",
    "        'valid_loss': [],\n",
    "        'train_top1_accuracy': [], \n",
    "        'valid_top1_accuracy': [],\n",
    "        'valid_loss_min': np.Inf,\n",
    "        'train_loss_min': np.Inf,\n",
    "        'best_acc': 0.0,\n",
    "    },\n",
    "    'readme': \"\",\n",
    "    'date': datetime.now().isoformat(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def increment_path(path, exist_ok=False, sep='', mkdir=False):\n",
    "    # 增加文件或目錄路徑，例如將 runs/exp 變成 runs/exp{sep}2, runs/exp{sep}3, ... 等等。\n",
    "    path = Path(path)  # 使路徑符合作業系統\n",
    "\n",
    "    if path.exists() and not exist_ok:\n",
    "        # 如果路徑存在且不允許重複，則進行增量操作\n",
    "        path, suffix = (path.with_suffix(''), path.suffix) if path.is_file() else (path, '')\n",
    "\n",
    "        for n in range(2, 9999):\n",
    "            p = f'{path}{sep}{n}{suffix}'  # 增加路徑\n",
    "            if not os.path.exists(p):  # 如果新路徑不存在，則跳出迴圈\n",
    "                break\n",
    "        path = Path(p)  # 將新路徑轉換為 Path 物件\n",
    "\n",
    "    if mkdir:\n",
    "        # 如果 mkdir 為 True，則創建新目錄\n",
    "        path.mkdir(parents=True, exist_ok=True)  \n",
    "\n",
    "    return str(path)  # 返回字符串形式的路徑\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def increment_path(path, exist_ok=False, sep='', mkdir=False):\n",
    "    # 增加文件或目錄路徑，例如將 runs/exp 變成 runs/exp{sep}2, runs/exp{sep}3, ... 等等。\n",
    "    path = Path(path)  # 使路徑符合作業系統\n",
    "\n",
    "    if path.exists() and not exist_ok:\n",
    "        # 如果路徑存在且不允許重複，則進行增量操作\n",
    "        path, suffix = (path.with_suffix(''), path.suffix) if path.is_file() else (path, '')\n",
    "\n",
    "        for n in range(2, 9999):\n",
    "            p = f'{path}{sep}{n}{suffix}'  # 增加路徑\n",
    "            if not os.path.exists(p):  # 如果新路徑不存在，則跳出迴圈\n",
    "                break\n",
    "        path = Path(p)  # 將新路徑轉換為 Path 物件\n",
    "\n",
    "    if mkdir:\n",
    "        # 如果 mkdir 為 True，則創建新目錄\n",
    "        path.mkdir(parents=True, exist_ok=True)  \n",
    "\n",
    "    return str(path)  # 返回字符串形式的路徑\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(ckpt, \n",
    "          save_dir, \n",
    "          model, \n",
    "          dataloaders, \n",
    "          optimizer, \n",
    "          lr_scheduler=None,\n",
    "          num_epochs=100,\n",
    "          dtype=torch.float32, \n",
    "          device='cpu'):\n",
    "\n",
    "    # 將儲存目錄轉換為 Path 物件\n",
    "    save_dir = Path(save_dir)\n",
    "    # 使用 increment_path 增加儲存目錄的路徑，避免複寫先前資料\n",
    "    save_dir = increment_path(\n",
    "        Path(save_dir), exist_ok=(False if ckpt['epoch'] == 0 else True), mkdir=True)\n",
    "\n",
    "    # 將模型移動到指定的裝置上，同步model的資料型別(要跟data 相同，不然會報錯)\n",
    "    model = model.to(dtype=dtype).to(device)\n",
    "\n",
    "    # 以存檔來初始化模型、優化器、學習率調度器和損失函數，如果程式碼意外停止，也能以ckpt來繼續訓練\n",
    "    model.load_state_dict(ckpt['model_state_dict'])\n",
    "    optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
    "    if (lr_scheduler):\n",
    "        lr_scheduler.load_state_dict(ckpt['lr_scheduler_state_dict'])\n",
    "    criterion = ckpt['criterion']\n",
    "\n",
    "    # 開始訓練循環\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "\n",
    "        # 初始化訓練和驗證的損失和準確率\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        train_top1 = 0.0\n",
    "        valid_top1 = 0.0\n",
    "        print(f\"running epoch: {ckpt['epoch'] + 1}\")\n",
    "\n",
    "        # 設定模型為訓練模式\n",
    "        model.train()\n",
    "        for x, y in tqdm(dataloaders['train']):\n",
    "            # 將優化器中的梯度清零\n",
    "            optimizer.zero_grad()\n",
    "            # 將輸入和標籤移動到指定的裝置上\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            # 前向傳播，計算預測和損失\n",
    "            predictions = model(x)\n",
    "            loss = criterion(predictions, y)\n",
    "            # 反向傳播，計算梯度\n",
    "            loss.backward()\n",
    "            # 使用優化器更新權重\n",
    "            optimizer.step()\n",
    "\n",
    "            # 累計訓練損失和準確率\n",
    "            train_loss += loss.item() * x.size(0)\n",
    "            train_top1 += torch.sum(torch.argmax(predictions, 1) == y).item()\n",
    "\n",
    "        # 設定模型為驗證模式\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for x, y in tqdm(dataloaders['valid']):\n",
    "                # 將輸入和標籤移動到指定的裝置上\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "\n",
    "                # 前向傳播，計算預測和損失\n",
    "                predictions = model(x)\n",
    "                loss = criterion(predictions, y)\n",
    "\n",
    "                # 累計驗證損失和準確率\n",
    "                valid_loss += loss.item() * x.size(0)\n",
    "                valid_top1 += torch.sum(torch.argmax(predictions, 1) == y).item()\n",
    "\n",
    "        # 如果有設置學習率調度器，則以驗證損失作為指標調整學習率\n",
    "        if (lr_scheduler):\n",
    "            lr_scheduler.step(valid_loss)\n",
    "\n",
    "        # 計算訓練和驗證的平均損失和準確率\n",
    "        train_loss = train_loss / len(dataloaders['train'].sampler)\n",
    "        valid_loss = valid_loss / len(dataloaders['valid'].sampler)\n",
    "        train_top1_acc = train_top1 / len(dataloaders['train'].sampler)\n",
    "        valid_top1_acc = valid_top1 / len(dataloaders['valid'].sampler)\n",
    "\n",
    "        # 將這一輪的訓練和驗證損失和準確率添加到歷史記錄中\n",
    "        ckpt['history']['train_loss'].append(train_loss)\n",
    "        ckpt['history']['valid_loss'].append(valid_loss)\n",
    "        ckpt['history']['train_top1_accuracy'].append(train_top1_acc)\n",
    "        ckpt['history']['valid_top1_accuracy'].append(valid_top1_acc)\n",
    "\n",
    "        # 打印這一輪的訓練和驗證損失和準確率\n",
    "        print(\n",
    "            f'Train loss     -> {train_loss:.6f}      \\\n",
    "                Validation loss     -> {valid_loss:.6f}')\n",
    "        print(\n",
    "            f'Train top1 acc -> {train_top1_acc:.6f}      \\\n",
    "                Validation top1 acc -> {valid_top1_acc:.6f}')\n",
    "\n",
    "        # 更新和保存檢查點信息\n",
    "        ckpt['epoch'] += 1\n",
    "        ckpt['model_state_dict'] = model.state_dict()\n",
    "        ckpt['optimizer_state_dict'] = optimizer.state_dict()\n",
    "        if (lr_scheduler):\n",
    "            ckpt['lr_scheduler_state_dict'] = lr_scheduler.state_dict()\n",
    "        ckpt['date'] = datetime.now().isoformat()\n",
    "\n",
    "        # 如果驗證損失降低，則保存模型\n",
    "        if valid_loss <= ckpt['history']['valid_loss_min']:\n",
    "            print(\n",
    "                f\"Validation loss decreased ({ckpt['history']['valid_loss_min']:.6f} \\\n",
    "                    --> {valid_loss:.6f}).  Saving model ...\")\n",
    "            ckpt['history']['valid_loss_min'] = valid_loss\n",
    "            torch.save(ckpt, str(Path(save_dir) / Path('valid_best.pth')))\n",
    "\n",
    "        # 每10輪保存一次檢查點\n",
    "        if (epoch) % 10 == 0:\n",
    "            torch.save(ckpt, str(Path(save_dir) / Path(f'ckpt_{epoch}.pth')))\n",
    "        torch.save(ckpt, str(Path(save_dir) / Path(f'last.pth')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running epoch: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cac9db569be43f0970f20f76d685e9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e824ac04825406093bbab42043a1b98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss     -> 0.951281                      Validation loss     -> 0.010852\n",
      "Train top1 acc -> 0.852812                      Validation top1 acc -> 0.998246\n",
      "Validation loss decreased (inf                     --> 0.010852).  Saving model ...\n",
      "running epoch: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53d0d543cc6c4311820e7cc2343ec498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1867db5abb3848e48f44a81afbcb90d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss     -> 0.019425                      Validation loss     -> 0.230428\n",
      "Train top1 acc -> 0.996924                      Validation top1 acc -> 0.984211\n",
      "running epoch: 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2126e653a474ad2a22285aacf233ec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51cb06ccccea40d4b55954f42e95cc26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss     -> 0.101968                      Validation loss     -> 0.815934\n",
      "Train top1 acc -> 0.982425                      Validation top1 acc -> 0.817544\n",
      "running epoch: 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4dbd35d6b594126aec3d3e1d7af9916",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1993d8922c394f5bafa44f689b38edc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss     -> 0.068589                      Validation loss     -> 0.008777\n",
      "Train top1 acc -> 0.985940                      Validation top1 acc -> 0.994737\n",
      "Validation loss decreased (0.010852                     --> 0.008777).  Saving model ...\n",
      "running epoch: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d94b8c69914846999a08502969374552",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3674cde6ea164803be3b49ac2abcdd4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss     -> 0.013105                      Validation loss     -> 0.000325\n",
      "Train top1 acc -> 0.997364                      Validation top1 acc -> 1.000000\n",
      "Validation loss decreased (0.008777                     --> 0.000325).  Saving model ...\n",
      "running epoch: 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56b119191da74d92aedfe82871476f36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4659b419ca2943e7a77c1a578e97bf83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss     -> 0.000730                      Validation loss     -> 0.000059\n",
      "Train top1 acc -> 1.000000                      Validation top1 acc -> 1.000000\n",
      "Validation loss decreased (0.000325                     --> 0.000059).  Saving model ...\n",
      "running epoch: 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fdea7131c2d4fd69ca46b024260f3a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b62d2a78066a40b585d94cc00a3a29da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss     -> 0.000771                      Validation loss     -> 0.000051\n",
      "Train top1 acc -> 1.000000                      Validation top1 acc -> 1.000000\n",
      "Validation loss decreased (0.000059                     --> 0.000051).  Saving model ...\n",
      "running epoch: 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6914415014344e2bbb8c25edf6e295b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15e31b1ac280443fa736ce83abf43daa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss     -> 0.000260                      Validation loss     -> 0.000024\n",
      "Train top1 acc -> 1.000000                      Validation top1 acc -> 1.000000\n",
      "Validation loss decreased (0.000051                     --> 0.000024).  Saving model ...\n",
      "running epoch: 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc2bb3522ae54ab9ae4338c4c22bfc19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff48f4a4ca4c48a3be160dbbf549fece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss     -> 0.000170                      Validation loss     -> 0.000020\n",
      "Train top1 acc -> 1.000000                      Validation top1 acc -> 1.000000\n",
      "Validation loss decreased (0.000024                     --> 0.000020).  Saving model ...\n",
      "running epoch: 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53b6041c5268436cb27c1fc07d34b7d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6726b8b30ec4677bacb0f83d877e8ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss     -> 0.000143                      Validation loss     -> 0.000017\n",
      "Train top1 acc -> 1.000000                      Validation top1 acc -> 1.000000\n",
      "Validation loss decreased (0.000020                     --> 0.000017).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "ckpt = train(ckpt, save_dir, model, dataloaders, optimizer, lr_scheduler=lr_scheduler, num_epochs=10, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(str(Path(fr\"./runs/exp9\") / Path('last.pth')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'acc')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 畫圖，只會畫前十個\n",
    "# n = len(ckpt['history']['train_top1_accuracy'])\n",
    "n = 10\n",
    "epochs = range(1, n + 1)\n",
    "plt.plot(epochs, ckpt['history']['train_top1_accuracy'][:n],label='train')\n",
    "plt.plot(epochs, ckpt['history']['valid_top1_accuracy'][:n],label='valid')\n",
    "# plt.xlim(50,)\n",
    "# plt.ylim(0,0.1)\n",
    "plt.legend()\n",
    "plt.title('mobilenet v2 top 1 accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('acc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchvision.transforms import ToTensor\n",
    "from PIL import Image\n",
    "\n",
    "def predict_and_save(model, folder_path, output_csv):\n",
    "    # 創建一個將 PIL 圖片轉換為張量的轉換器\n",
    "    transform = ToTensor()\n",
    "    # 創建一個空的列表來保存結果\n",
    "    results = []\n",
    "\n",
    "    # 遍歷指定目錄下的所有文件\n",
    "    for filename in os.listdir(folder_path):\n",
    "        # 如果文件是 PNG 圖片\n",
    "        if filename.endswith(\".png\"): \n",
    "            # 獲取圖片的完整路徑\n",
    "            image_path = os.path.join(folder_path, filename)\n",
    "            # 打開並將圖片轉換為 RGB\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            # 將圖片轉換為張量並增加一個維度以匹配模型的輸入\n",
    "            image_tensor = transform(image).unsqueeze(0)\n",
    "\n",
    "            # 如果 CUDA 可用，則將模型和圖片張量移至 GPU\n",
    "            if torch.cuda.is_available():  \n",
    "                model = model.cuda()\n",
    "                image_tensor = image_tensor.cuda()\n",
    "\n",
    "            # 將模型設為評估模式\n",
    "            model.eval()\n",
    "            # 不計算梯度，因為這是推論，不需要反向傳播\n",
    "            with torch.no_grad():\n",
    "                # 進行預測\n",
    "                prediction = model(image_tensor)\n",
    "                # 從預測結果中獲取最大概率類別的索引\n",
    "                category = prediction.argmax().item()\n",
    "                # 將文件名和預測類別添加到結果列表中\n",
    "                results.append((filename, category))\n",
    "\n",
    "    # 將結果列表轉換為 DataFrame\n",
    "    results_df = pd.DataFrame(results, columns=[\"filename\", \"category\"])\n",
    "    # 將結果 DataFrame 寫入 CSV 文件\n",
    "    results_df.to_csv(output_csv, index=False)\n",
    "\n",
    "\n",
    "# 使用方法\n",
    "predict_and_save(model, \"data/music_test\", \"output.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
