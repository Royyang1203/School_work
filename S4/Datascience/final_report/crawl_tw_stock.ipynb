{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba881fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import csv\n",
    "import time\n",
    "import string\n",
    "import logging\n",
    "import requests\n",
    "import argparse\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from os import mkdir\n",
    "from os.path import isdir\n",
    "\n",
    "class Crawler():\n",
    "    def __init__(self, prefix=\"data\"):\n",
    "        ''' Make directory if not exist when initialize '''\n",
    "        if not isdir(prefix):\n",
    "            mkdir(prefix)\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def _clean_row(self, row):\n",
    "        ''' Clean comma and spaces '''\n",
    "        for index, content in enumerate(row):\n",
    "            row[index] = re.sub(\",\", \"\", content.strip())\n",
    "        return row\n",
    "\n",
    "    def _record(self, stock_id, row):\n",
    "        ''' Save row to csv file '''\n",
    "        f = open('{}/{}.csv'.format(self.prefix, stock_id), 'a')\n",
    "        cw = csv.writer(f, lineterminator='\\n')\n",
    "        cw.writerow(row)\n",
    "        f.close()\n",
    "\n",
    "    def _get_tse_data(self, date_tuple):\n",
    "        date_str = '{0}{1:02d}{2:02d}'.format(date_tuple[0], date_tuple[1], date_tuple[2])\n",
    "        url = 'http://www.twse.com.tw/exchangeReport/MI_INDEX'\n",
    "\n",
    "        query_params = {\n",
    "            'date': date_str,\n",
    "            'response': 'json',\n",
    "            'type': 'ALL',\n",
    "            '_': str(round(time.time() * 1000) - 500)\n",
    "        }\n",
    "\n",
    "        # Get json data\n",
    "        page = requests.get(url, params=query_params)\n",
    "\n",
    "        if not page.ok:\n",
    "            logging.error(\"Can not get TSE data at {}\".format(date_str))\n",
    "            return\n",
    "\n",
    "        content = page.json()\n",
    "\n",
    "        # For compatible with original data\n",
    "        date_str_mingguo = '{0}/{1:02d}/{2:02d}'.format(date_tuple[0] - 1911, date_tuple[1], date_tuple[2])\n",
    "\n",
    "        for data in content['data5']:\n",
    "            sign = '-' if data[9].find('green') > 0 else ''\n",
    "            row = self._clean_row([\n",
    "                date_str_mingguo, # 日期\n",
    "                data[2], # 成交股數\n",
    "                data[4], # 成交金額\n",
    "                data[5], # 開盤價\n",
    "                data[6], # 最高價\n",
    "                data[7], # 最低價\n",
    "                data[8], # 收盤價\n",
    "                sign + data[10], # 漲跌價差\n",
    "                data[3], # 成交筆數\n",
    "            ])\n",
    "\n",
    "            self._record(data[0].strip(), row)\n",
    "\n",
    "    def _get_otc_data(self, date_tuple):\n",
    "        date_str = '{0}/{1:02d}/{2:02d}'.format(date_tuple[0] - 1911, date_tuple[1], date_tuple[2])\n",
    "        ttime = str(int(time.time()*100))\n",
    "        url = 'http://www.tpex.org.tw/web/stock/aftertrading/daily_close_quotes/stk_quote_result.php?l=zh-tw&d={}&_={}'.format(date_str, ttime)\n",
    "        page = requests.get(url)\n",
    "\n",
    "        if not page.ok:\n",
    "            logging.error(\"Can not get OTC data at {}\".format(date_str))\n",
    "            return\n",
    "\n",
    "        result = page.json()\n",
    "\n",
    "        if result['reportDate'] != date_str:\n",
    "            logging.error(\"Get error date OTC data at {}\".format(date_str))\n",
    "            return\n",
    "\n",
    "        for table in [result['mmData'], result['aaData']]:\n",
    "            for tr in table:\n",
    "                row = self._clean_row([\n",
    "                    date_str,\n",
    "                    tr[8], # 成交股數\n",
    "                    tr[9], # 成交金額\n",
    "                    tr[4], # 開盤價\n",
    "                    tr[5], # 最高價\n",
    "                    tr[6], # 最低價\n",
    "                    tr[2], # 收盤價\n",
    "                    tr[3], # 漲跌價差\n",
    "                    tr[10] # 成交筆數\n",
    "                ])\n",
    "                self._record(tr[0], row)\n",
    "\n",
    "\n",
    "    def get_data(self, date_tuple):\n",
    "        print('Crawling {}'.format(date_tuple))\n",
    "        self._get_tse_data(date_tuple)\n",
    "        self._get_otc_data(date_tuple)\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0943f034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logging\n",
    "    if not os.path.isdir('log'):\n",
    "        os.makedirs('log')\n",
    "    logging.basicConfig(filename='log/crawl-error.log',\n",
    "        level=logging.ERROR,\n",
    "        format='%(asctime)s\\t[%(levelname)s]\\t%(message)s',\n",
    "        datefmt='%Y/%m/%d %H:%M:%S')\n",
    "\n",
    "    # Get arguments\n",
    "    parser = argparse.ArgumentParser(description='Crawl data at assigned day')\n",
    "    parser.add_argument('day', type=int, nargs='*',\n",
    "        help='assigned day (format: YYYY MM DD), default is today')\n",
    "    parser.add_argument('-b', '--back', action='store_true',\n",
    "        help='crawl back from assigned day until 2004/2/11')\n",
    "    parser.add_argument('-c', '--check', action='store_true',\n",
    "        help='crawl back 10 days for check data')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Day only accept 0 or 3 arguments\n",
    "    if len(args.day) == 0:\n",
    "        first_day = datetime.today()\n",
    "    elif len(args.day) == 3:\n",
    "        first_day = datetime(args.day[0], args.day[1], args.day[2])\n",
    "    else:\n",
    "        parser.error('Date should be assigned with (YYYY MM DD) or none')\n",
    "        return\n",
    "\n",
    "    crawler = Crawler()\n",
    "\n",
    "    # If back flag is on, crawl till 2004/2/11, else crawl one day\n",
    "    if args.back or args.check:\n",
    "        # otc first day is 2007/04/20\n",
    "        # tse first day is 2004/02/11\n",
    "\n",
    "        last_day = datetime(2004, 2, 11) if args.back else first_day - timedelta(10)\n",
    "        max_error = 5\n",
    "        error_times = 0\n",
    "\n",
    "        while error_times < max_error and first_day >= last_day:\n",
    "            try:\n",
    "                crawler.get_data((first_day.year, first_day.month, first_day.day))\n",
    "                error_times = 0\n",
    "            except:\n",
    "                date_str = first_day.strftime('%Y/%m/%d')\n",
    "                logging.error('Crawl raise error {}'.format(date_str))\n",
    "                error_times += 1\n",
    "                continue\n",
    "            finally:\n",
    "                first_day -= timedelta(1)\n",
    "    else:\n",
    "        crawler.get_data((first_day.year, first_day.month, first_day.day))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
